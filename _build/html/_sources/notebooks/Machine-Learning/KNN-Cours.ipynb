{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=0p0o5cmgLdE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce chapitre, on voit deux algorithmes pour l’intelligence artificielle : les k plus\n",
    "proches voisins (KNN k nearest neighbors) et les k-moyennes (k means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme KNN: Cours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On considère le problème suivant : on dispose N objets sur lesquels on a mesuré différentes valeurs.\\\n",
    "Les mêmes mesures ont été faites sur chacun des objets.\n",
    "- La valeur de N est très grande.\n",
    "- Ces objets se répartissent selon T classes, avec T petit. On sait dans quel classe va chaque objet que l’on a mesuré.\n",
    "- Maintenant, on ajoute un nouvel objet sur lequel on a fait les mêmes mesures. \n",
    "- On souhaite lui associer une classe.\n",
    "- De quelle classe est-il le plus proche ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On parle ainsi **D’APPRENTISSAGE SUPERVISÉ** . En effet, on apprend sur un jeu des données déjà classées comment classer les nouvelles données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exemple 1.*\n",
    "- Ce type d’algorithme est très utilisé en intelligence artificielle particulièrement dans le commerce en ligne.\n",
    "- On a des informations sur N personnes (âge, sexe, habitude de navigation, etc.). On a classé leurs centres d’intérêts selon T types.\n",
    "- On peut alors proposer automatiquement à un nouvel individu des publicités adap-\n",
    "tées en le classant automatiquement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exemple 2.*\n",
    "- Pour faire des diagnostics automatiques, On dispose de mesures médicales sur N individus pour lesquels on sait si il y a ou pas une tumeur cancéreuse, c'-à-d que l’on a répartis dans deux classes : malade / sain.\n",
    "- À partir de ces données, on peut déterminer automatiquement si un individu doit être considéré comme malade ou sain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La notion de distance:\n",
    "- Pour donner un sens à la classe la plus proche, cela suppose a minima de choisir une manière de mesurer les écarts entre les objets, c-à-d d’avoir une distance sur les mesures associées à aux objets.\n",
    "- Cela n’est pas du tout évident. D’autant plus que les mesures peuvent prendre\n",
    "diverses formes.\n",
    "- Le choix de la distance est donc critique pour la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=lambda A,B: sum([abs(a-b) for a,b in zip(A,B)])\n",
    "d2=lambda A,B: np.sqrt(sum([(a-b)**2 for a,b in zip(A,B)]))\n",
    "d_inf=lambda A,B: max([abs(a-b) for a,b in zip(A,B)])\n",
    "d2_np=lambda A,B: np.sqrt(sum((A-B)**2))\n",
    "d1_np=lambda A,B: sum(abs(A-B))\n",
    "d_inf_np=lambda A,B: max(abs(A-B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principe de l’algorithme\n",
    "**Mathématiquement**, \n",
    "- On dispose donc d’un ensemble $E$ assez complexe (l’ensemble des mesures), $E$ est muni d’une distance $d$.\n",
    "- On dispose d’un ensemble C de classe de cardinal T .\n",
    "- On dispose donc de N éléments de $E \\times C$ (les mesures faites sur les objets et les classes associées). On les notes $(x_j , c_j)_{j\\in [1, N]}$ . \n",
    "- Ainsi, $x_j$ est la liste des mesures faites sur l’objet $j$, et $c_j$ la classe associée.\n",
    "- **Problème** On considère de plus un élément y de E, non classé et on veut lui associer une classe.\n",
    "\n",
    "**Exemple :**\n",
    "Comme problème jouet, on considère donc que $E = \\mathbb{R}^n$ et que l’on prend la distance\n",
    "euclidienne. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’algorithme des KNN consiste à :\n",
    "- mesurer les distance $d(y, x_j )$ pour $j ∈ [|1, N|]$,\n",
    "- déterminer les k plus proches voisins de y. Les k éléments les plus proches de\n",
    "y sont notés : $x_{i_1} , x_{i_2} , . . . , x_{i_k}$ .\n",
    "- Choisir pour classe de y la classe la plus fréquente parmi celles de $(x_{i_1} , x_{i_2} , . . . , x_{i_k} )$\n",
    "\n",
    "\n",
    "Le choix du paramètre k (nombre de voisins que l’on prend en compte) est critique.\\\n",
    "Il n’est pas facile de régler ce paramètre et de l’interpréter.\\\n",
    "D’autre part, on peut avoir plusieurs classes avec le même effectif parmi les k voisins.\\\n",
    "Dans ce cas, l’algorithme ne permet pas de conclure et on attribue généralement arbitrairement une classe au nouvel objet y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place de chaque étape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesure des distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence donc par créer une liste des distances : $d(y, x_j )$ pour $j \\in [[1, N]]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Distances(y ,LearningData,d=d2) :\n",
    "    \"\"\"\n",
    "    entrée :\n",
    "        lX = liste d'objets ( é l é ment de R^n) de longueur N\n",
    "        y = objet ( é l é ment de R^n)\n",
    "        on dispose d'une fonction d\n",
    "        qui mesure la distance entre objets\n",
    "    sortie :\n",
    "        lD = liste de float= liste des distances d(y,xk ) pour k dans [|1 , N |]\n",
    "    \"\"\"\n",
    "    return [d(y,x) for x in LearningData]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déterminer les k plus proches voisins\n",
    "Ensuite, il s’agit de déterminer les k plus proches voisins.\\\n",
    "Pour cela, on se contente d’utiliser l’algorithme de tri rapide en etulisant la fonction **sorted**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on voit que l’on veut les indices des éléments les plus proches, pas les valeurs.\\\n",
    "En effet, ce n’est pas trier les distances qui nous intéressent mais trier les points.\\\n",
    "C'est pour cette raison nous n'allos pas utiliser **sorted** directement sur les distances mais nou allons utiliser la fonction **np.argsort** qui retourne la liste des indices des elements qui rendent la liste des distances une liste triée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les distances:  [5, 10, 9, 11, 0.3]\n",
      "les indices:  [4 0 2 1 3]\n",
      "4 est l'indice de 0.3 dans la liste distances, ce qui représente la distance minimale,\n",
      "aisni 3 est l'indice de  la distance maximale qui est 11\n"
     ]
    }
   ],
   "source": [
    "## Exemple \n",
    "distances=[5,10,9,11,0.3]\n",
    "indices=np.argsort(distances)\n",
    "print(\"les distances: \",distances)\n",
    "print(\"les indices: \",indices)\n",
    "print(\"{} est l'indice de {} dans la liste distances, ce qui représente la distance minimale,\\naisni {} est l'indice de  la distance maximale qui est {}\".format(indices[0],distances[indices[0]],indices[-1],distances[indices[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(Ditanceslist,k):\n",
    "    \"\"\"\n",
    "    entrée :\n",
    "            Ditanceslist = liste de float\n",
    "               = liste des distances entre l'objet à placer \n",
    "               y et chacun des objets de la liste d'apprentissages\n",
    "            k = int = nombre de voisins à prendre en compte \n",
    "    sortie : lKNN = liste de int de longueur K\n",
    "      = liste des indices des K plus proches voisins .\n",
    "\"\"\"\n",
    "    return np.argsort(Ditanceslist)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_version2(Ditanceslist,k):\n",
    "    triee=sorted(enumerate(Ditanceslist),key=lambda e:e[1])\n",
    "    return [e[0] for e in triee][:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choisir la classe la plus fréquente\n",
    "Pour déterminer la classe la plus fréquente, il faut compter combien de fois est présente chaque classe.\n",
    "\n",
    "Pour cela, on parcourt les k plus proches avec un compteur de longueur T (le nombre de classe). \n",
    "\n",
    "Pour chaque élément lu, on incrémente le compteur correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compter(TargetValues):\n",
    "    compteur={}\n",
    "    for e in TargetValues:\n",
    "        if e not in compteur:\n",
    "            compteur[e]=1\n",
    "        else:\n",
    "            compteur[e]+=1\n",
    "    return compteur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, un simple algorithme de recherche de maximum donne la classe la plus fréquente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plus_commun(compteur):\n",
    "    max(compteur,key=lambda e:compteur[e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédire la Sortie d'une nouvelle donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(y,LearningData,LearningTarget,d=d2_np,k=11):\n",
    "    Ditanceslist=distances(y,LearningData,d)\n",
    "    voisins=KNN(Ditanceslist,k)\n",
    "    return Plus_commun(Compter(LearningTarget[voisins]))\n",
    "    #return Counter(LearningTarget[voisins]).most_common()[0]\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion\n",
    "La matrice de confusion permet de mesurer la qualité du système de classification.\\\n",
    "Pour tester la qualité, on prend M objets dont on connaît la classification.\\\n",
    "Cette classification est qualifiée de certaine.\\\n",
    "On applique l’algorithme à chacun de ses M objets et on note la classification obtenue (dite classification estimée).\\\n",
    "On met ces résultats dans une matrice : chaque ligne de la matrice correspond à une classe certaine, chaque colonne a une classe estimée. Ainsi ligne i et colonne j on met le nombre d’éléments qui ont été classifiée dans la classe j alors qu’ils sont dans la classe i.\\\n",
    "Si la classification était parfaite, alors seule la diagonale aurait des éléments non nuls.\\\n",
    "On considère qu’une classification est de qualité lorsque chaque ligne contient 95% de ses valeurs sur l’élément diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(TestData,TestTarget,model):\n",
    "    LearningData,LearningTarget,d,k=model\n",
    "    types=list(set(TestTarget))\n",
    "    n=len(types)\n",
    "    matrice=np.zeros((n,n))\n",
    "    for i in range(len(TestData)):\n",
    "        p=predict(TestData[i],LearningData,LearningTarget,d,k)\n",
    "        matrice[TestTarget[i],p]+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae0f79b32b2845dac7e94ce8e1e5e879cbbee5c3210b1fdfbe8967149bf7e5fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
